import type { PerformanceMonitor } from '../_shared/performanceMonitor.ts';
import type { BBLLMTokenUsage } from '../_shared/types.ts';
import { type OpenAICompatConfig, OpenAICompatHandler } from './handlerOpenAICompat.ts';
//import { Chat } from 'openai/resources';
import type OpenAI from 'openai';

export type Model =
	//| (string & {})
	| 'gpt-4-0125-preview'
	| 'gpt-4-1106-preview'
	| 'gpt-4-vision-preview'
	| 'gpt-4'
	| 'gpt-3.5-turbo-0125'
	| 'gpt-3.5-turbo';

interface OpenAIHandlerConfig extends OpenAICompatConfig {
	model: Model;
}

type OpenAITokenUsage = OpenAI.CompletionUsage;

// OpenAI provider handler
export class OpenAIHandler extends OpenAICompatHandler<OpenAITokenUsage> {
	constructor(config: OpenAIHandlerConfig, monitor: PerformanceMonitor) {
		super(config, monitor);
	}

	protected override getProviderName(): string {
		return 'openai';
	}

	protected override transformUsage(usage: OpenAITokenUsage | undefined): BBLLMTokenUsage {
		const cachedTokens = usage?.prompt_tokens_details?.cached_tokens ?? 0;
		const totalPromptTokens = usage?.prompt_tokens ?? 0;

		return {
			// Tokens we had to process (total minus cached)
			inputTokens: Math.max(0, totalPromptTokens - cachedTokens),
			outputTokens: usage?.completion_tokens ?? 0,
			// We're reading from cache, not creating cache entries
			cacheCreationInputTokens: 0,
			// Tokens we got from cache
			cacheReadInputTokens: cachedTokens,
		};

		/*
		 * https://platform.openai.com/docs/api-reference/chat/object
		 * Usage statistics for the completion request.
		 *
		 * completion_tokens
		 * Number of tokens in the generated completion.
		 *
		 * prompt_tokens
		 * Number of tokens in the prompt.
		 *
		 * total_tokens
		 * Total number of tokens used in the request (prompt + completion).
		 *
		 * completion_tokens_details:
		 * Breakdown of tokens used in a completion.
		 *
		 * 	  accepted_prediction_tokens
		 * 	  When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion.
		 *
		 * 	  audio_tokens
		 * 	  Audio input tokens generated by the model.
		 *
		 * 	  reasoning_tokens
		 * 	  Tokens generated by the model for reasoning.
		 *
		 * 	  rejected_prediction_tokens
		 * 	  When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits.
		 *
		 * prompt_tokens_details:
		 * Breakdown of tokens used in the prompt.
		 *
		 * 	  audio_tokens
		 * 	  Audio input tokens present in the prompt.
		 *
		 * 	  cached_tokens
		 * 	  Cached tokens present in the prompt.
		 */
	}
}
